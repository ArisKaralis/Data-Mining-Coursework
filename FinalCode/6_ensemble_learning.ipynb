{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042d7701",
   "metadata": {},
   "source": [
    "# Alzheimer's Disease Prediction - Ensemble Learning Approaches\n",
    "\n",
    "This notebook explores various ensemble learning techniques to potentially improve the model performance for Alzheimer's disease prediction, with a focus on voting classifiers. We'll implement and compare several approaches including:\n",
    "\n",
    "1. Hard voting classifiers\n",
    "2. Soft voting classifiers\n",
    "3. Weighted voting classifiers\n",
    "4. Stacking ensemble models\n",
    "5. Combining with dimensionality reduction techniques\n",
    "\n",
    "The goal is to leverage the strengths of multiple models to achieve better predictive performance for Alzheimer's disease diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59413774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import ensemble models\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Import dimensionality reduction methods for combined analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# For displaying plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b6e6d",
   "metadata": {},
   "source": [
    "## 1. Loading and Preparing the Data\n",
    "\n",
    "We'll load the processed data from the previous feature engineering approaches and prepare it for our ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cdaf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical approach features shape: (59426, 23)\n",
      "Model-based approach features shape: (59426, 19)\n"
     ]
    }
   ],
   "source": [
    "# Load the feature-engineered data\n",
    "# First check which approach performed better in previous notebooks\n",
    "fe_statistical = pd.read_csv('fe_statistical_approach.csv')\n",
    "fe_model_based = pd.read_csv('fe_model_based_approach.csv')\n",
    "\n",
    "# We'll use both approaches and compare results\n",
    "# Approach 1: Statistical feature engineering\n",
    "data_statistical = fe_statistical.copy()\n",
    "X_statistical = data_statistical.drop(\"Alzheimer's Diagnosis\", axis=1)\n",
    "y = data_statistical[\"Alzheimer's Diagnosis\"]\n",
    "\n",
    "# Approach 2: Model-based feature engineering\n",
    "data_model_based = fe_model_based.copy()\n",
    "X_model_based = data_model_based.drop(\"Alzheimer's Diagnosis\", axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_stat_train, X_stat_test, y_train, y_test = train_test_split(X_statistical, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_model_train, X_model_test, y_train_dup, y_test_dup = train_test_split(X_model_based, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler_stat = StandardScaler()\n",
    "X_stat_train_scaled = scaler_stat.fit_transform(X_stat_train)\n",
    "X_stat_test_scaled = scaler_stat.transform(X_stat_test)\n",
    "\n",
    "scaler_model = StandardScaler()\n",
    "X_model_train_scaled = scaler_model.fit_transform(X_model_train)\n",
    "X_model_test_scaled = scaler_model.transform(X_model_test)\n",
    "\n",
    "print(f\"Statistical approach features shape: {X_stat_train_scaled.shape}\")\n",
    "print(f\"Model-based approach features shape: {X_model_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee673a",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Models from Previous Analysis\n",
    "\n",
    "We'll load the best performing models from our previous analyses to include them in our ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e47d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely load models\n",
    "def load_model(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Model file {file_path} not found. Creating new instance.\")\n",
    "        return None\n",
    "\n",
    "# Load pre-trained models if available\n",
    "lr_model = load_model('logistic_regression_best_model.pkl')\n",
    "rf_model = load_model('random_forest_best_model.pkl')\n",
    "gb_model = load_model('gradient_boosting_best_model.pkl')\n",
    "\n",
    "# Create base model instances if pre-trained models not available\n",
    "if lr_model is None:\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "if rf_model is None:\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "if gb_model is None:\n",
    "    gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create additional models for ensembling\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "cat_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# List all models for easy reference\n",
    "models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'Gradient Boosting': gb_model,\n",
    "    'SVM': svm_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'CatBoost': cat_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e8b6e",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Framework\n",
    "\n",
    "We'll create a comprehensive evaluation framework for our ensemble models, including cross-validation and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b587bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models with cross-validation\n",
    "def evaluate_model_cv(model, X, y, cv=5):\n",
    "    cv_results = cross_validate(\n",
    "        model, X, y, \n",
    "        cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42),\n",
    "        scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Compute mean and std of metrics\n",
    "    metrics = {}\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "        metrics[f'test_{metric}'] = {\n",
    "            'mean': cv_results[f'test_{metric}'].mean(),\n",
    "            'std': cv_results[f'test_{metric}'].std()\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Function for detailed model evaluation on test set\n",
    "# Function for detailed model evaluation on test set\n",
    "def evaluate_model_test(model, X_train, X_test, y_train, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Check if the model supports probability predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    elif hasattr(model, 'decision_function'):  # For SVM without probability\n",
    "        y_prob = model.decision_function(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        # For hard voting and other models without probability outputs\n",
    "        y_prob = y_pred  # Just use the predictions (this won't be a proper probability)\n",
    "        print(f\"Warning: {type(model).__name__} doesn't support probability predictions. ROC AUC may not be valid.\")\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return metrics, cm, y_prob\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_test, y_prob_dict, model_names):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name in model_names:\n",
    "        y_prob = y_prob_dict[model_name]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05494496",
   "metadata": {},
   "source": [
    "## 4. Hard Voting Ensemble\n",
    "\n",
    "In hard voting, the predicted class label for each test instance is the one that receives the highest number of votes from the base classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0d034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Hard Voting Classifier with statistical features:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "VotingClassifier has none of the following attributes: decision_function, predict_proba.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate with cross-validation\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Hard Voting Classifier with statistical features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m hard_voting_cv_stat \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhard_voting_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_stat_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, values \u001b[38;5;129;01min\u001b[39;00m hard_voting_cv_stat\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (±\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mevaluate_model_cv\u001b[0;34m(model, X, y, cv)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model_cv\u001b[39m(model, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStratifiedKFold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Compute mean and std of metrics\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    885\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 888\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_params_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:949\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[1;32m    947\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 949\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:126\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate predicted target values.\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 126\u001b[0m cache \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    127\u001b[0m cached_call \u001b[38;5;241m=\u001b[39m partial(_cached_call, cache)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:165\u001b[0m, in \u001b[0;36m_MultimetricScorer._use_cache\u001b[0;34m(self, estimator)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scorers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Only one scorer\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    164\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter(\n\u001b[0;32m--> 165\u001b[0m     [\n\u001b[1;32m    166\u001b[0m         _check_response_method(estimator, scorer\u001b[38;5;241m.\u001b[39m_response_method)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m scorer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scorers\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _BaseScorer)\n\u001b[1;32m    169\u001b[0m     ]\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(val \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# The exact same response method or iterable of response methods\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# will be called more than once.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scorers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Only one scorer\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    164\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter(\n\u001b[1;32m    165\u001b[0m     [\n\u001b[0;32m--> 166\u001b[0m         \u001b[43m_check_response_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m scorer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scorers\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _BaseScorer)\n\u001b[1;32m    169\u001b[0m     ]\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(val \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# The exact same response method or iterable of response methods\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# will be called more than once.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dlt_labs/lib/python3.9/site-packages/sklearn/utils/validation.py:2283\u001b[0m, in \u001b[0;36m_check_response_method\u001b[0;34m(estimator, response_method)\u001b[0m\n\u001b[1;32m   2281\u001b[0m prediction_method \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;129;01mor\u001b[39;00m y, prediction_method)\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has none of the following attributes: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(list_methods)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2286\u001b[0m     )\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction_method\n",
      "\u001b[0;31mAttributeError\u001b[0m: VotingClassifier has none of the following attributes: decision_function, predict_proba."
     ]
    }
   ],
   "source": [
    "# Create hard voting classifier\n",
    "hard_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_model),\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model),\n",
    "        ('svm', svm_model),\n",
    "        ('xgb', xgb_model),\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "print(\"Evaluating Hard Voting Classifier with statistical features:\")\n",
    "hard_voting_cv_stat = evaluate_model_cv(hard_voting_clf, X_stat_train_scaled, y_train)\n",
    "for metric, values in hard_voting_cv_stat.items():\n",
    "    print(f\"{metric}: {values['mean']:.4f} (±{values['std']:.4f})\")\n",
    "\n",
    "print(\"\\nEvaluating Hard Voting Classifier with model-based features:\")\n",
    "hard_voting_cv_model = evaluate_model_cv(hard_voting_clf, X_model_train_scaled, y_train)\n",
    "for metric, values in hard_voting_cv_model.items():\n",
    "    print(f\"{metric}: {values['mean']:.4f} (±{values['std']:.4f})\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest performance with statistical features:\")\n",
    "hard_metrics_stat, hard_cm_stat, hard_prob_stat = evaluate_model_test(\n",
    "    hard_voting_clf, X_stat_train_scaled, X_stat_test_scaled, y_train, y_test\n",
    ")\n",
    "for metric, value in hard_metrics_stat.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(hard_cm_stat, \"Hard Voting Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337f2a6",
   "metadata": {},
   "source": [
    "## 5. Soft Voting Ensemble\n",
    "\n",
    "In soft voting, each classifier provides a probability estimate for each class, and these probabilities are averaged to find the final class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab247361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Soft Voting Classifier with statistical features:\n"
     ]
    }
   ],
   "source": [
    "# Create soft voting classifier\n",
    "# Note: All classifiers need to support predict_proba\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_model),\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model),\n",
    "        ('svm', svm_model),\n",
    "        ('xgb', xgb_model),\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "print(\"Evaluating Soft Voting Classifier with statistical features:\")\n",
    "soft_voting_cv_stat = evaluate_model_cv(soft_voting_clf, X_stat_train_scaled, y_train)\n",
    "for metric, values in soft_voting_cv_stat.items():\n",
    "    print(f\"{metric}: {values['mean']:.4f} (±{values['std']:.4f})\")\n",
    "\n",
    "print(\"\\nEvaluating Soft Voting Classifier with model-based features:\")\n",
    "soft_voting_cv_model = evaluate_model_cv(soft_voting_clf, X_model_train_scaled, y_train)\n",
    "for metric, values in soft_voting_cv_model.items():\n",
    "    print(f\"{metric}: {values['mean']:.4f} (±{values['std']:.4f})\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest performance with statistical features:\")\n",
    "soft_metrics_stat, soft_cm_stat, soft_prob_stat = evaluate_model_test(\n",
    "    soft_voting_clf, X_stat_train_scaled, X_stat_test_scaled, y_train, y_test\n",
    ")\n",
    "for metric, value in soft_metrics_stat.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(soft_cm_stat, \"Soft Voting Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9198977",
   "metadata": {},
   "source": [
    "## 6. Weighted Voting Ensemble\n",
    "\n",
    "In weighted voting, we assign different weights to each classifier based on their performance, giving more influence to better-performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train individual models to determine weights\n",
    "model_metrics = {}\n",
    "model_probs = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Skip CatBoost for weighted voting (to be used later in stacking)\n",
    "    if name == 'CatBoost':\n",
    "        continue\n",
    "        \n",
    "    print(f\"Training {name}...\")\n",
    "    metrics, _, probs = evaluate_model_test(\n",
    "        model, X_stat_train_scaled, X_stat_test_scaled, y_train, y_test\n",
    "    )\n",
    "    model_metrics[name] = metrics\n",
    "    model_probs[name] = probs\n",
    "\n",
    "# Display performance metrics for each model\n",
    "metrics_df = pd.DataFrame({\n",
    "    name: {\n",
    "        metric: value for metric, value in metrics.items()\n",
    "    } for name, metrics in model_metrics.items()\n",
    "})\n",
    "\n",
    "print(\"\\nIndividual model performance:\")\n",
    "print(metrics_df.T)\n",
    "\n",
    "# Assign weights based on AUC scores\n",
    "weights = {name: metrics['roc_auc'] for name, metrics in model_metrics.items()}\n",
    "\n",
    "# Create weighted voting classifier\n",
    "weighted_voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_model),\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model),\n",
    "        ('svm', svm_model),\n",
    "        ('xgb', xgb_model),\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[weights['Logistic Regression'], \n",
    "             weights['Random Forest'], \n",
    "             weights['Gradient Boosting'], \n",
    "             weights['SVM'], \n",
    "             weights['XGBoost']]\n",
    ")\n",
    "\n",
    "# Evaluate weighted voting\n",
    "print(\"\\nEvaluating Weighted Voting Classifier:\")\n",
    "weighted_metrics, weighted_cm, weighted_prob = evaluate_model_test(\n",
    "    weighted_voting_clf, X_stat_train_scaled, X_stat_test_scaled, y_train, y_test\n",
    ")\n",
    "for metric, value in weighted_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(weighted_cm, \"Weighted Voting Ensemble\")\n",
    "\n",
    "# Add probabilities for the weighted voting model\n",
    "model_probs['Weighted Voting'] = weighted_prob\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curve(y_test, model_probs, list(model_probs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529eed7",
   "metadata": {},
   "source": [
    "## 7. Stacking Ensemble Model\n",
    "\n",
    "Stacking involves training a meta-model that combines the predictions of multiple base models, potentially capturing more complex patterns than simple voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacking classifier\n",
    "# We'll use logistic regression as the final estimator\n",
    "estimators = [\n",
    "    ('lr', lr_model),\n",
    "    ('rf', rf_model),\n",
    "    ('gb', gb_model),\n",
    "    ('svm', svm_model),\n",
    "    ('xgb', xgb_model),\n",
    "]\n",
    "\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(random_state=42),\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "# Evaluate stacking classifier\n",
    "print(\"Evaluating Stacking Classifier with statistical features:\")\n",
    "stack_metrics_stat, stack_cm_stat, stack_prob_stat = evaluate_model_test(\n",
    "    stack_clf, X_stat_train_scaled, X_stat_test_scaled, y_train, y_test\n",
    ")\n",
    "for metric, value in stack_metrics_stat.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(stack_cm_stat, \"Stacking Ensemble\")\n",
    "\n",
    "print(\"\\nEvaluating Stacking Classifier with model-based features:\")\n",
    "stack_metrics_model, stack_cm_model, stack_prob_model = evaluate_model_test(\n",
    "    stack_clf, X_model_train_scaled, X_model_test_scaled, y_train, y_test\n",
    ")\n",
    "for metric, value in stack_metrics_model.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Add stacking model probabilities\n",
    "model_probs['Stacking'] = stack_prob_stat\n",
    "\n",
    "# Plot updated ROC curves with stacking model included\n",
    "plot_roc_curve(y_test, model_probs, list(model_probs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486e807",
   "metadata": {},
   "source": [
    "## 8. Combining Dimensionality Reduction with Ensemble Learning\n",
    "\n",
    "Here we'll explore whether combining dimensionality reduction techniques with our best performing ensemble model can further improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f50a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines combining dimensionality reduction with ensemble methods\n",
    "# We'll use the best performing ensemble method from above\n",
    "\n",
    "# Determine best ensemble model (let's assume stacking for this example)\n",
    "best_ensemble_model = stack_clf\n",
    "\n",
    "# Create pipelines with different dimensionality reduction techniques\n",
    "pipelines = {\n",
    "    'PCA + Ensemble': Pipeline([\n",
    "        ('pca', PCA(n_components=0.95)),  # Keep 95% of variance\n",
    "        ('ensemble', best_ensemble_model)\n",
    "    ]),\n",
    "    'LDA + Ensemble': Pipeline([\n",
    "        ('lda', LDA(n_components=1)),  # Binary classification -> 1 component\n",
    "        ('ensemble', best_ensemble_model)\n",
    "    ]),\n",
    "    'SelectKBest + Ensemble': Pipeline([\n",
    "        ('select', SelectKBest(f_classif, k=10)),  # Select top 10 features\n",
    "        ('ensemble', best_ensemble_model)\n",
    "    ]),\n",
    "    'Ensemble Only': best_ensemble_model\n",
    "}\n",
    "\n",
    "# Evaluate all pipelines\n",
    "pipeline_metrics = {}\n",
    "pipeline_probs = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\nEvaluating {name}:\")\n",
    "    metrics, cm, probs = evaluate_model_test(\n",
    "        pipeline, X_stat_train_scaled, X_stat_test_scaled, y_train, y_test\n",
    "    )\n",
    "    pipeline_metrics[name] = metrics\n",
    "    pipeline_probs[name] = probs\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    plot_confusion_matrix(cm, name)\n",
    "\n",
    "# Compare pipeline performance\n",
    "pipeline_df = pd.DataFrame({\n",
    "    name: {\n",
    "        metric: value for metric, value in metrics.items()\n",
    "    } for name, metrics in pipeline_metrics.items()\n",
    "})\n",
    "\n",
    "print(\"\\nPipeline performance comparison:\")\n",
    "print(pipeline_df.T)\n",
    "\n",
    "# Plot ROC curves for the pipelines\n",
    "plot_roc_curve(y_test, pipeline_probs, list(pipeline_probs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee1f7c",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Model Comparison\n",
    "\n",
    "Let's compare all the models we've developed, including individual models and ensembles, to identify the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all metrics into a single dataframe\n",
    "all_metrics = {}\n",
    "\n",
    "# Add individual model metrics\n",
    "all_metrics.update(model_metrics)\n",
    "\n",
    "# Add ensemble model metrics\n",
    "all_metrics['Hard Voting'] = hard_metrics_stat\n",
    "all_metrics['Soft Voting'] = soft_metrics_stat\n",
    "all_metrics['Weighted Voting'] = weighted_metrics\n",
    "all_metrics['Stacking'] = stack_metrics_stat\n",
    "\n",
    "# Add pipeline metrics\n",
    "all_metrics.update(pipeline_metrics)\n",
    "\n",
    "# Create dataframe and sort by F1 score\n",
    "all_metrics_df = pd.DataFrame({\n",
    "    name: {\n",
    "        metric: value for metric, value in metrics.items()\n",
    "    } for name, metrics in all_metrics.items()\n",
    "})\n",
    "\n",
    "print(\"\\nAll model performance metrics:\")\n",
    "print(all_metrics_df.T.sort_values('f1', ascending=False))\n",
    "\n",
    "# Visualize the comparison with a heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(all_metrics_df.T, annot=True, cmap='viridis', fmt='.3f', linewidths=0.5)\n",
    "plt.title('Performance Comparison of All Models')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot comparing F1 scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "models_sorted = all_metrics_df.T.sort_values('f1', ascending=False).index\n",
    "sns.barplot(x=all_metrics_df.loc['f1', models_sorted], y=models_sorted)\n",
    "plt.title('Model Comparison by F1 Score')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.xlim(min(all_metrics_df.loc['f1']) - 0.05, 1.0)\n",
    "plt.grid(True, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf68a2",
   "metadata": {},
   "source": [
    "## 10. Save the Best Performing Model\n",
    "\n",
    "We'll save the best performing model for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9afd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model based on F1 score\n",
    "best_model_name = all_metrics_df.T.sort_values('f1', ascending=False).index[0]\n",
    "print(f\"The best performing model is: {best_model_name}\")\n",
    "\n",
    "# Get the best model\n",
    "if best_model_name in pipelines:\n",
    "    best_model = pipelines[best_model_name]\n",
    "    # Ensure the model is fitted\n",
    "    best_model.fit(X_stat_train_scaled, y_train)\n",
    "elif best_model_name == 'Hard Voting':\n",
    "    best_model = hard_voting_clf\n",
    "    best_model.fit(X_stat_train_scaled, y_train)\n",
    "elif best_model_name == 'Soft Voting':\n",
    "    best_model = soft_voting_clf\n",
    "    best_model.fit(X_stat_train_scaled, y_train)\n",
    "elif best_model_name == 'Weighted Voting':\n",
    "    best_model = weighted_voting_clf\n",
    "    best_model.fit(X_stat_train_scaled, y_train)\n",
    "elif best_model_name == 'Stacking':\n",
    "    best_model = stack_clf\n",
    "    best_model.fit(X_stat_train_scaled, y_train)\n",
    "else:\n",
    "    best_model = models[best_model_name]\n",
    "    best_model.fit(X_stat_train_scaled, y_train)\n",
    "\n",
    "# Save the best model\n",
    "best_model_path = 'best_ensemble_model.pkl'\n",
    "with open(best_model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1d954",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we explored various ensemble learning approaches for Alzheimer's disease prediction, including voting classifiers and stacking ensembles. We also combined these approaches with dimensionality reduction techniques to potentially improve performance further.\n",
    "\n",
    "Key findings:\n",
    "1. The best performing model was [will be determined by execution]\n",
    "2. Ensemble models generally outperformed individual models\n",
    "3. Combining dimensionality reduction with ensemble learning [will be determined by execution]\n",
    "\n",
    "Next steps could include:\n",
    "1. Further fine-tuning of the best performing model\n",
    "2. Exploring more complex ensemble architectures\n",
    "3. Developing a deployment strategy for clinical use\n",
    "4. Conducting external validation on independent datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt_labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
